{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embededing based Text classification \n",
    "\n",
    "## by Yash Tripathi\n",
    "\n",
    "\n",
    "In this document we will be following the Text classification method using Word embededing and DNN Models to classify the data and get the Accuracy Result.\n",
    "It will be then compared with the other Predictions done by my other team memnbers and then we will choose the most aplicable model from the set.\n",
    "\n",
    "Team Members:-\n",
    "[@Yash Tripahti](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Hitesh Goyal](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Ashwin Iyer](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Rishma ](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Rahul Dash](https://drkakku.github.io/GoV2/todo.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The basic Pipeline that i will be following in this case will be :- \n",
    "\n",
    "- Split the data into text (X) and labels (Y)\n",
    "- Preprocess X\n",
    "- Create a word embedding matrix from X\n",
    "- Create a tensor input from X\n",
    "- Train a deep learning model using the tensor inputs and labels (Y)\n",
    "- Make predictions on new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in /Users/yashtripathi/miniforge3/envs/nlp/lib/python3.10/site-packages (0.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "### Get Data ##\n",
    "train = pd.read_csv(\"../input/train.csv\",index_col=0)\n",
    "test = pd.read_csv(\"../input/test.csv\",index_col=0)\n",
    "validation = pd.read_csv(\"../input/validation.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorise(df):\n",
    "    df.label =  pd.factorize(df['label'])[0].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorise(train)\n",
    "factorise(test)\n",
    "factorise(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43465</th>\n",
       "      <td>white house petition calling ariz primary inve...</td>\n",
       "      <td>petition calling obama administration investig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13278</th>\n",
       "      <td>civil war ii fourth turning intensifying part</td>\n",
       "      <td>posted november jimq part one article laid cas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5764</th>\n",
       "      <td>homeland security chairman hillary mishandling...</td>\n",
       "      <td>homeland security chairman hillary mishandling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21826</th>\n",
       "      <td>trump criticizes congress s move weakening eth...</td>\n",
       "      <td>washington reuters us presidentelect donald tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60667</th>\n",
       "      <td>yahoo taps x desktop search</td>\n",
       "      <td>search engine giant yahoo tapped pasadenabased...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "43465  white house petition calling ariz primary inve...   \n",
       "13278     civil war ii fourth turning intensifying part    \n",
       "5764   homeland security chairman hillary mishandling...   \n",
       "21826  trump criticizes congress s move weakening eth...   \n",
       "60667                        yahoo taps x desktop search   \n",
       "\n",
       "                                                    text  label  \n",
       "43465  petition calling obama administration investig...      0  \n",
       "13278  posted november jimq part one article laid cas...      0  \n",
       "5764   homeland security chairman hillary mishandling...      0  \n",
       "21826  washington reuters us presidentelect donald tr...      1  \n",
       "60667  search engine giant yahoo tapped pasadenabased...      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this cleaning function to clean the dataset \n",
    "\n",
    "It takes a lot of time and may be unnessary as it will most probabily not contribute much to the accuracy\n",
    "```py\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(x, spell=spell):\n",
    "    \"\"\"correct the missplled words of a given tweet\"\"\"\n",
    "    x = x.split()\n",
    "    misspelled = spell.unknown(x)\n",
    "    result = map(lambda word : spell.correction(word) if word in  misspelled else word, x)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def tweets_cleaning(x, correct_spelling=True, remove_emojis=True, remove_stop_words=True):\n",
    "    \"\"\"Apply function to a clean a tweet\"\"\"\n",
    "    x = x.lower().strip()\n",
    "    # romove urls\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    x = url.sub(r'',x)\n",
    "    # remove html tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    x = html.sub(r'',x)\n",
    "    # remove punctuation\n",
    "    operator = str.maketrans('','',string.punctuation) #????\n",
    "    x = x.translate(operator)\n",
    "    if correct_spelling:\n",
    "        x = correct_spellings(x)\n",
    "    if remove_emojis:\n",
    "        x = x.encode('ascii', 'ignore').decode('utf8').strip()\n",
    "    if remove_stop_words:\n",
    "        x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    return x\n",
    "\n",
    "\n",
    "## APPLY the cleaning function to the text column\n",
    "df['cleaned_tweets'] = df['text'].apply(tweets_cleaning)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,trainY = xySplit(train)\n",
    "testX,testY = xySplit(test)\n",
    "validationX,validationY = xySplit(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gert a distribution of the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6457</th>\n",
       "      <td>ashton kutcher rescues sex trafficking victims...</td>\n",
       "      <td>christopher ashton kutcher wellknown figure am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52107</th>\n",
       "      <td>florida crowd attacks police officer attemptin...</td>\n",
       "      <td>citizenry respect law law enforcement officers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55294</th>\n",
       "      <td>oracle profit rises software demand</td>\n",
       "      <td>san francisco reuters oracle corp tuesday repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27913</th>\n",
       "      <td>erdogan pope say phone call attempts change je...</td>\n",
       "      <td>ankara reuters turkish president tayyip erdoga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59719</th>\n",
       "      <td>allrounder mcgrath</td>\n",
       "      <td>glenn mcgrath thoroughbred fast bowler decade ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "6457   ashton kutcher rescues sex trafficking victims...   \n",
       "52107  florida crowd attacks police officer attemptin...   \n",
       "55294                oracle profit rises software demand   \n",
       "27913  erdogan pope say phone call attempts change je...   \n",
       "59719                                 allrounder mcgrath   \n",
       "\n",
       "                                                    text  \n",
       "6457   christopher ashton kutcher wellknown figure am...  \n",
       "52107  citizenry respect law law enforcement officers...  \n",
       "55294  san francisco reuters oracle corp tuesday repo...  \n",
       "27913  ankara reuters turkish president tayyip erdoga...  \n",
       "59719  glenn mcgrath thoroughbred fast bowler decade ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = pd.concat([trainX,validationX])\n",
    "np.shape(total_words)\n",
    "total_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "# Limit on the number of features to K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. \n",
    "# Sequences longer than this will be truncated.\n",
    "# and less than it will be padded\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, train_texts):\n",
    "        self.train_texts = train_texts\n",
    "        self.tokenizer = Tokenizer(num_words=TOP_K)\n",
    "        \n",
    "    def train_tokenize(self):\n",
    "        # Get max sequence length.\n",
    "        max_length = len(max(self.train_texts , key=len))\n",
    "        self.max_length = min(max_length, MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "        # Create vocabulary with training texts.\n",
    "        self.tokenizer.fit_on_texts(self.train_texts)\n",
    "        \n",
    "    def vectorize_input(self, tweets):\n",
    "        # Vectorize training and validation texts.\n",
    "        \n",
    "        tweets = self.tokenizer.texts_to_sequences(tweets)\n",
    "        # Fix sequence length to max value. Sequences shorter than the length are\n",
    "        # padded in the beginning and sequences longer are truncated\n",
    "        # at the beginning.\n",
    "        tweets = sequence.pad_sequences(tweets, maxlen=self.max_length, truncating='post',padding='post')\n",
    "        return tweets\n",
    "\n",
    "tokenizer = CustomTokenizer(train_texts = total_words['text'])\n",
    "# fit o the train\n",
    "tokenizer.train_tokenize()\n",
    "tokenized_train = tokenizer.vectorize_input(trainX['text'])\n",
    "tokenized_val = tokenizer.vectorize_input(validationX['text'])\n",
    "tokenized_test = tokenizer.vectorize_input(testX['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import tqdm\n",
    "import requests\n",
    "import zipfile\n",
    "URL = \"http://nlp.stanford.edu/data/glove.42B.300d.zip\"\n",
    "\n",
    "def fetch_data(url=URL, target_file='../input/embedings/glove.zip', delete_zip=False):\n",
    "    #if the dataset already exists exit\n",
    "    if os.path.isfile(target_file):\n",
    "        print(\"datasets already downloded :) \")\n",
    "        return\n",
    "\n",
    "    #download (large) zip file\n",
    "    #for large https request on stream mode to avoid out of memory issues\n",
    "    #see : http://masnun.com/2016/09/18/python-using-the-requests-module-to-download-large-files-efficiently.html\n",
    "    print(\"**************************\")\n",
    "    print(\"  Downloading zip file\")\n",
    "    print(\"  >_<  Please wait >_< \")\n",
    "    print(\"**************************\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    #read chunk by chunk\n",
    "    handle = open(target_file, \"wb\")\n",
    "    for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n",
    "        if chunk:  \n",
    "            handle.write(chunk)\n",
    "    handle.close()  \n",
    "    print(\"  Download completed ;) :\") \n",
    "    #extract zip_file\n",
    "    zf = zipfile.ZipFile(target_file)\n",
    "    print(\"1. Extracting {} file\".format(target_file))\n",
    "    zf.extractall()\n",
    "    if delete_zip:\n",
    "        print(\"2. Deleting {} file\".format(dataset_name+\".zip\"))\n",
    "        os.remove(path=zip_file)\n",
    "\n",
    "fetch_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236079/236079 [00:00<00:00, 689569.34it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_file = \"../input/embedings/glove.6B.300d.txt\"\n",
    "import tqdm\n",
    "\n",
    "EMBEDDING_VECTOR_LENGTH = 300 # = 300\n",
    "def construct_embedding_matrix(glove_file, word_index):\n",
    "    embedding_dict = {}\n",
    "    with open(glove_file,'r') as f:\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            # get the word\n",
    "            word=values[0]\n",
    "            if word in word_index.keys():\n",
    "                # get the vector\n",
    "                vector = np.asarray(values[1:], 'float32')\n",
    "                embedding_dict[word] = vector\n",
    "    ###  oov words (out of vacabulary words) will be mapped to 0 vectors\n",
    "\n",
    "    num_words=len(word_index)+1\n",
    "    #initialize it to 0\n",
    "    embedding_matrix=np.zeros((num_words, EMBEDDING_VECTOR_LENGTH))\n",
    "\n",
    "    for word,i in tqdm.tqdm(word_index.items()):\n",
    "        if i < num_words:\n",
    "            vect=embedding_dict.get(word, [])\n",
    "            if len(vect)>0:\n",
    "                embedding_matrix[i] = vect[:EMBEDDING_VECTOR_LENGTH]\n",
    "    return embedding_matrix\n",
    "  \n",
    "embedding_matrix =  construct_embedding_matrix(glove_file, tokenizer.tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(len(tokenizer.tokenizer.word_index)+1, # number of unique tokens\n",
    "                    EMBEDDING_VECTOR_LENGTH, #number of features\n",
    "                    embeddings_initializer=Constant(embedding_matrix), # initialize \n",
    "                    input_length=MAX_SEQUENCE_LENGTH, \n",
    "                    trainable=False)\n",
    "                    \n",
    "\n",
    "model.add(embedding)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimzer = Adam(clipvalue=0.5)\n",
    "model.compile(optimizer=optimzer,loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1242/1242 [==============================] - 315s 253ms/step - loss: 0.1929 - acc: 0.9292 - val_loss: 0.1912 - val_acc: 0.9253\n",
      "Epoch 2/20\n",
      "1242/1242 [==============================] - 312s 251ms/step - loss: 0.1799 - acc: 0.9337 - val_loss: 0.1412 - val_acc: 0.9533\n",
      "Epoch 3/20\n",
      "1242/1242 [==============================] - 318s 256ms/step - loss: 0.1495 - acc: 0.9479 - val_loss: 0.1335 - val_acc: 0.9557\n",
      "Epoch 4/20\n",
      "1242/1242 [==============================] - 317s 255ms/step - loss: 0.1410 - acc: 0.9508 - val_loss: 0.1242 - val_acc: 0.9595\n",
      "Epoch 5/20\n",
      "1242/1242 [==============================] - 311s 251ms/step - loss: 0.1301 - acc: 0.9546 - val_loss: 0.1245 - val_acc: 0.9573\n",
      "Epoch 6/20\n",
      "1242/1242 [==============================] - 316s 254ms/step - loss: 0.1213 - acc: 0.9576 - val_loss: 0.1205 - val_acc: 0.9593\n",
      "Epoch 7/20\n",
      "1242/1242 [==============================] - 308s 248ms/step - loss: 0.1170 - acc: 0.9593 - val_loss: 0.1305 - val_acc: 0.9590\n",
      "Epoch 8/20\n",
      "1242/1242 [==============================] - 308s 248ms/step - loss: 0.1115 - acc: 0.9607 - val_loss: 0.1182 - val_acc: 0.9605\n",
      "Epoch 9/20\n",
      "1242/1242 [==============================] - 304s 244ms/step - loss: 0.1064 - acc: 0.9629 - val_loss: 0.1183 - val_acc: 0.9608\n",
      "Epoch 10/20\n",
      "1242/1242 [==============================] - 301s 242ms/step - loss: 0.1025 - acc: 0.9635 - val_loss: 0.1145 - val_acc: 0.9628\n",
      "Epoch 11/20\n",
      "1242/1242 [==============================] - 306s 246ms/step - loss: 0.0970 - acc: 0.9653 - val_loss: 0.1145 - val_acc: 0.9626\n",
      "Epoch 12/20\n",
      "1242/1242 [==============================] - 301s 242ms/step - loss: 0.0918 - acc: 0.9672 - val_loss: 0.1150 - val_acc: 0.9614\n",
      "Epoch 13/20\n",
      "1242/1242 [==============================] - 309s 249ms/step - loss: 0.0890 - acc: 0.9673 - val_loss: 0.1291 - val_acc: 0.9563\n",
      "Epoch 14/20\n",
      "1242/1242 [==============================] - 305s 246ms/step - loss: 0.0863 - acc: 0.9692 - val_loss: 0.1171 - val_acc: 0.9619\n",
      "Epoch 15/20\n",
      " 766/1242 [=================>............] - ETA: 5:35 - loss: 0.0826 - acc: 0.9697"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/6jmf4pkj4yd7v5smf85g9mt00000gn/T/ipykernel_46722/2324577678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(tokenized_train, trainY, \n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# fit the model\n",
    "history = model.fit(tokenized_train, trainY, \n",
    "\n",
    "                    batch_size=32, \n",
    "                    epochs=20, \n",
    "                    validation_data=(tokenized_val,validationY), \n",
    "                    verbose=1,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/yash/lstm64_half/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/yash/lstm64_half/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x2cd01aaa0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Message third_party.py.keras.protobuf.SavedMetadata exceeds maximum protobuf size of 2GB: 2408393954",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/6jmf4pkj4yd7v5smf85g9mt00000gn/T/ipykernel_46722/2003148614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/yash/lstm64_half.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../models/yash/lstm64_half\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/nlp/lib/python3.10/site-packages/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m     99\u001b[0m   with tf.io.gfile.GFile(\n\u001b[1;32m    100\u001b[0m       os.path.join(filepath, constants.SAVED_METADATA_PATH), \"wb\") as w:\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Message third_party.py.keras.protobuf.SavedMetadata exceeds maximum protobuf size of 2GB: 2408393954"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/yash/lstm64_half.h5\")\n",
    "\n",
    "model.save(\"../models/yash/lstm64_half\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4280, 11336,  4758,  1241,    43,  3741,   617,  2338,  1502,\n",
       "          10,  4500,  5770,  6644,  3354,  4975,   200,   309,  1120,\n",
       "       15035,  3005,    80,  9350,  1397,   378,  1866,    75,  1241,\n",
       "       10824,   729,  3971,  1028, 11172,  1453,   606,  4550,     7,\n",
       "         647,   935,  1294,  1431,    41,  4220,  1852,  3416,   258,\n",
       "        4220,   213,  3782,  6212,  2045], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335/335 [==============================] - 23s 68ms/step - loss: 6.3764 - acc: 0.0426\n"
     ]
    }
   ],
   "source": [
    "evaluate = model.evaluate(tokenized_test,testY,callbacks=[tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "331a24a01f83adadd46afead2dc489e28f6af802c207fc8776f5eca2295cb1f5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
