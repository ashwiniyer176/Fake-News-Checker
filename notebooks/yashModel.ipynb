{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embededing based Text classification \n",
    "\n",
    "## by Yash Tripathi\n",
    "\n",
    "\n",
    "In this document we will be following the Text classification method using Word embededing and DNN Models to classify the data and get the Accuracy Result.\n",
    "It will be then compared with the other Predictions done by my other team memnbers and then we will choose the most aplicable model from the set.\n",
    "\n",
    "Team Members:-\n",
    "[@Yash Tripahti](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Hitesh Goyal](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Ashwin Iyer](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Rishma ](https://drkakku.github.io/GoV2/todo.html)\n",
    "[@Rahul Dash](https://drkakku.github.io/GoV2/todo.html)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The basic Pipeline that i will be following in this case will be :- \n",
    "\n",
    "- Split the data into text (X) and labels (Y)\n",
    "- Preprocess X\n",
    "- Create a word embedding matrix from X\n",
    "- Create a tensor input from X\n",
    "- Train a deep learning model using the tensor inputs and labels (Y)\n",
    "- Make predictions on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = pd.read_csv(\"../input/train.csv\",index_col=0)\n",
    "testDf = pd.read_csv(\"../input/test.csv\",index_col=0)\n",
    "validationDf = pd.read_csv(\"../input/validation.csv\",index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6457</th>\n",
       "      <td>ashton kutcher rescues sex trafficking victims...</td>\n",
       "      <td>christopher ashton kutcher wellknown figure am...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52107</th>\n",
       "      <td>florida crowd attacks police officer attemptin...</td>\n",
       "      <td>citizenry respect law law enforcement officers...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55294</th>\n",
       "      <td>oracle profit rises software demand</td>\n",
       "      <td>san francisco reuters oracle corp tuesday repo...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27913</th>\n",
       "      <td>erdogan pope say phone call attempts change je...</td>\n",
       "      <td>ankara reuters turkish president tayyip erdoga...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59719</th>\n",
       "      <td>allrounder mcgrath</td>\n",
       "      <td>glenn mcgrath thoroughbred fast bowler decade ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "6457   ashton kutcher rescues sex trafficking victims...   \n",
       "52107  florida crowd attacks police officer attemptin...   \n",
       "55294                oracle profit rises software demand   \n",
       "27913  erdogan pope say phone call attempts change je...   \n",
       "59719                                 allrounder mcgrath   \n",
       "\n",
       "                                                    text label  \n",
       "6457   christopher ashton kutcher wellknown figure am...  FAKE  \n",
       "52107  citizenry respect law law enforcement officers...  FAKE  \n",
       "55294  san francisco reuters oracle corp tuesday repo...  REAL  \n",
       "27913  ankara reuters turkish president tayyip erdoga...  REAL  \n",
       "59719  glenn mcgrath thoroughbred fast bowler decade ...  REAL  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do some basic NLP preprosessing of the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(string:str ,\n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "\n",
    "    #Cleaning URL's\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+',\"\",string)\n",
    "\n",
    "    #cleaning the HTML elements\n",
    "    string = re.sub(r'<.*?>',\"\",string)\n",
    "\n",
    "    # removing the puntuations\n",
    "\n",
    "    for x in string.lower():\n",
    "        if x in punctuations:\n",
    "            string = string.replace(x,\"\")\n",
    "\n",
    "    #converting text to lower\n",
    "\n",
    "    string  = string.lower()\n",
    "\n",
    "    # removing the stop words\n",
    "    string = \" \".join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    #Cleaning the white space\n",
    "    string = re.sub(r'\\s+',\" \",string=string).strip()\n",
    "\n",
    "    return string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValDfX , ValDfY = validationDf[\"title\"] , validationDf[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    \"\"\"\n",
    "    A class to read the word embedding file and to create the word embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, vector_dimension):\n",
    "        self.path = path \n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def get_embedding_index(self):\n",
    "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
    "        return embeddings_index\n",
    "\n",
    "    def create_embedding_matrix(self, tokenizer, max_features):\n",
    "        \"\"\"\n",
    "        A method to create the embedding matrix\n",
    "        \"\"\"\n",
    "        model_embed = self.get_embedding_index()\n",
    "\n",
    "        embedding_matrix = np.zeros((max_features + 1, self.vector_dimension))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > max_features:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "                except:\n",
    "                    continue\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "     \n",
    "\n",
    "class TextToTensor():\n",
    "\n",
    "\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def string_to_tensor(self, string_list: list) -> list:\n",
    "        \"\"\"\n",
    "        A method to convert a string list to a tensor for a deep learning model\n",
    "        \"\"\"    \n",
    "        string_list = self.tokenizer.texts_to_sequences(string_list)\n",
    "        string_list = pad_sequences(string_list, maxlen=self.max_len)\n",
    "        \n",
    "        return string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(ValDfX)\n",
    "# Getting the longest sentence\n",
    "max_len = np.max([len(text.split()) for text in ValDfX])\n",
    "# Converting to tensor\n",
    "TextToTensor_instance = TextToTensor(\n",
    "tokenizer=tokenizer,\n",
    "max_len=max_len\n",
    ")\n",
    "X_train_NN = TextToTensor_instance.string_to_tensor(ValDfX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_path = '../input/embedings/glove.6B.300d.txt'\n",
    "embed_dim = 300\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(ValDfX)\n",
    "# Creating the embedding matrix\n",
    "embedding = Embeddings(embed_path, embed_dim)\n",
    "embedding_matrix = embedding.create_embedding_matrix(tokenizer, len(tokenizer.word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10699, 42)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "# Converting to tensor\n",
    "TextToTensor_instance = TextToTensor(\n",
    "tokenizer=tokenizer,\n",
    "max_len=max_len\n",
    ")\n",
    "X_train_NN = TextToTensor_instance.string_to_tensor(ValDfX)\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "  input_dim=14742, \n",
    "  output_dim=300, \n",
    "  input_length=max_len,\n",
    "  weights=[embedding_matrix]))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(X_train_NN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 300)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "477b20ca26d9de7454a1f9ac0072cbc227bf784c809cf08aa3c711c5906afb95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
